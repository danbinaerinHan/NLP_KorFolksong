{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os \n",
    "import torch.nn \n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn \n",
    "from tqdm.auto import tqdm\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip final_ttp_audio.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!upzip final_tts_audio.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor():\n",
    "  def __init__(self, filtered_txt_paths, each_lyric, sorted_txt_paths, teach_forcing_prob=0.8):\n",
    "    self.filtered_txt_paths = filtered_txt_paths\n",
    "    self.each_lyric = each_lyric\n",
    "    self.sorted_txt_paths = sorted_txt_paths\n",
    "    self.teach_forcing_prob = teach_forcing_prob\n",
    "\n",
    "  def read_text(self, txt_path):\n",
    "    with open(txt_path, 'r') as f:\n",
    "      text = f.read()\n",
    "    return text\n",
    "  \n",
    "  def list_read_text(self, txt_path_list):\n",
    "    all_txt = []\n",
    "    #print(txt_path_list)\n",
    "    for i in txt_path_list:\n",
    "      pth = self.sorted_txt_paths[i]\n",
    "      with open(pth, 'r') as f:\n",
    "        text = f.read()\n",
    "        all_txt.append(text)\n",
    "        \n",
    "    return '\\n '.join(all_txt) + '\\n'\n",
    "      \n",
    "\n",
    "  def get_random_sentence(self, idx_num, random_mode=False):\n",
    "    #if teach_forcing == True:\n",
    "    \n",
    "    if random_mode == False:\n",
    "\n",
    "      key_with_value = [key for key, value in self.each_lyric.items() if value == (idx_num)]\n",
    "      if len(key_with_value) > 50:\n",
    "        key_with_value = random.sample(key_with_value, 50)\n",
    "        return key_with_value\n",
    "      \n",
    "      \n",
    "    else:\n",
    "      key_with_value = [random.randint(0, len(self.filtered_txt_paths)) for _ in range(30)]\n",
    "\n",
    "    return key_with_value\n",
    "  \n",
    "  def random_position(self, text_list):\n",
    "    random.shuffle(text_list)\n",
    "    return text_list\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.each_lyric)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    txt_path = self.filtered_txt_paths[idx]\n",
    "    idx_num = self.each_lyric[int(txt_path.stem.split('lyric')[1])] # 몇번 곡인지 반환 \n",
    "    input_text = self.read_text(txt_path)\n",
    "    \n",
    "    around_sentence = self.get_random_sentence(idx_num)\n",
    "    around_sentence = self.random_position(around_sentence)\n",
    "    around_sentence =  self.list_read_text(around_sentence)\n",
    "    \n",
    "\n",
    "    if random.random() < self.teach_forcing_prob: # True \n",
    "      if isinstance(idx_num, tuple):\n",
    "        rand_idx = random.randint(0, 1)\n",
    "        idx_num = idx_num[rand_idx]\n",
    "        around_sentence = self.get_random_sentence(idx_num, False)\n",
    "        around_sentence = self.random_position(around_sentence)\n",
    "        around_sentence = self.list_read_text(around_sentence) \n",
    "        \n",
    "      else:\n",
    "        around_sentence = self.get_random_sentence(idx_num, False)\n",
    "        around_sentence = self.random_position(around_sentence)\n",
    "        around_sentence =  self.list_read_text(around_sentence)\n",
    "\n",
    "    else: \n",
    "      around_sentence = self.get_random_sentence(idx_num, True)\n",
    "      around_sentence = self.random_position(around_sentence)\n",
    "      around_sentence =  self.list_read_text(around_sentence)\n",
    "\n",
    "\n",
    "    return input_text, around_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(self, txt_path):\n",
    "    with open(txt_path, 'r') as f:\n",
    "      text = f.read()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "a = [1,2,3,4,5]\n",
    "random.shuffle(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('무슨 사주가 기박하여 세상살이 살고 보니', '놈산 시상 내 못살고\\n한두달에 못하다가\\n마른자리 느그 입헤\\n열달에 배 슬어서\\n어매 어매 우리 어매\\n신세 신세 내신세야\\n나난시에 남도 낳건마는\\n세상살이 보소\\n겉만 타먼 놈이 안디\\n죽자니는 청춘이고\\n나타난것 무엇이냐\\n속만 타먼 놈이 아냐\\n세상이 그렇구나\\n뭣을 묵고 나를 나서\\n삼식이식 기렀건만\\n육지 백판 이내수고 어디가고\\n내가슴에 불은\\n놈난시에 나도 낳고\\n진자리는 내가 눕고\\n아들딸을 낳건마는\\n나도 세상 살어날때\\n세상살이 살고 보니\\n신세한탄 한이 없네\\n살자니는 고생이로고나\\n자식들아 느그 날때\\n무슨 사주가 기박하여\\n우리 어매 울아부지 날 날적에\\n애야 서라 내 팔자야\\n마당갓에 모닥불만', 'lyric893-4')\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random \n",
    "import pickle\n",
    "class TextProcessor():\n",
    "  def __init__(self, id_list, lyric_path, each_lyric, random_ratio=0.8):\n",
    "    self.lyric_path = Path(lyric_path)\n",
    "    self.filtered_id = id_list\n",
    "    self.lyrics_list = [lyric_path+i+'.txt' for i in self.filtered_id]\n",
    "    self.each_lyric_list = self.read_text(each_lyric).split('\\n')\n",
    "    self.random_ratio = random_ratio\n",
    "    # self.teach_forcing_prob = teach_forcing_prob\n",
    "  \n",
    "  def read_text(self, txt_path):\n",
    "    with open(txt_path, 'r') as f:\n",
    "      text = f.read()\n",
    "    return text\n",
    "  \n",
    "  def get_all_sentence_list(self):\n",
    "    all_sentence = []\n",
    "    for i in self.lyrics_list:\n",
    "      all_sentence.append(self.read_text(i))\n",
    "    return list(set(all_sentence))\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.lyrics_list)\n",
    "  \n",
    "  def get_random_sentence(self, lyric_name, random_mode=False):\n",
    "    # lyric_name = 'lyric0-1.txt'\n",
    "    if random.random() < self.random_ratio:\n",
    "      random_mode = True\n",
    "    if random_mode == False:\n",
    "      lyric_index = int(lyric_name.split('-')[0][5:])\n",
    "      lyrics = self.each_lyric_list[lyric_index]\n",
    "      lyrics = lyrics.split(', ')\n",
    "      if len(lyrics) > 30:\n",
    "        lyrics = random.sample(lyrics, 30)\n",
    "      # print(lyrics)\n",
    "      random.shuffle(lyrics)\n",
    "      \n",
    "    else: \n",
    "      lyrics = random.sample(self.get_all_sentence_list(), 30)\n",
    "      \n",
    "    return lyrics\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    txt_path = self.lyrics_list[idx]\n",
    "    lyric_name = txt_path.split('/')[-1][:-4]\n",
    "    # print(lyric_name)\n",
    "    input_text = self.read_text(txt_path)\n",
    "    random.seed(0)\n",
    "    output_sentence = self.get_random_sentence(lyric_name)\n",
    "    output_sentence = '\\n'.join(output_sentence)\n",
    "    return input_text, output_sentence, lyric_name\n",
    "  \n",
    "\n",
    "\n",
    "lyric_path = '/home/daewoong/userdata/danbi/final_lyrics_data/'\n",
    "each_lyric = '/home/daewoong/userdata/danbi/each_song_lyrics.txt'\n",
    "filtered_id_list = pickle.load(open('/home/daewoong/userdata/danbi/thirty_second_filtered_id.pkl', 'rb'))\n",
    "\n",
    "processor = TextProcessor(filtered_id_list, lyric_path, each_lyric)\n",
    "print(processor[1302])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torchaudio\n",
    "def filter_length(audio_path, max_len=30):\n",
    "  audio_list = [str(i) for i in audio_path.rglob('*.wav')]\n",
    "  filtered_audio_list = []\n",
    "  for i in audio_list:\n",
    "    audio, sr = torchaudio.load(i)\n",
    "    if len(audio)/sr <= float(max_len):\n",
    "      filtered_audio_list.append(i)\n",
    "  return filtered_audio_list\n",
    "audio_path = Path('/home/daewoong/userdata/danbi/final_tts_audio')\n",
    "filtered_audio_list = filter_length(audio_path)\n",
    "print(len(filtered_audio_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_path = '/home/daewoong/userdata/danbi/final_lyrics_data/'\n",
    "each_lyric = '/home/daewoong/userdata/danbi/each_song_lyrics.txt'\n",
    "filtered_id_list = pickle.load(open('/home/daewoong/userdata/danbi/thirty_second_filtered_id.pkl', 'rb'))\n",
    "\n",
    "processor = TextProcessor(filtered_id_list, lyric_path, each_lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ..\n",
    "# with open('thirty_second_filtered_id.pkl', 'wb') as f:\n",
    "#   pickle.dump(filtered_id_list, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel spec으로 바꿔놓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lyric3696-1',\n",
       " 'lyric5730-4',\n",
       " 'lyric7157-9',\n",
       " 'lyric4166-0',\n",
       " 'lyric5710-4',\n",
       " 'lyric5855-6',\n",
       " 'lyric4112-28',\n",
       " 'lyric2841-2',\n",
       " 'lyric5429-2',\n",
       " 'lyric5472-25']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/daewoong/userdata/danbi/thirty_second_filtered_id.pkl', 'rb') as f:\n",
    "  filtered_id_list = pickle.load(f)\n",
    "filtered_id_list[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\", language=\"ko\", task=\"transcribe\", predict_timestamps=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audio = self.processor.feature_extractor(audio[0] , sampling_rate=sr, padding_value = 0.0, return_tensors=\"pt\", return_attention_mask = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\", language=\"ko\", task=\"transcribe\", predict_timestamps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "audio_path = '/home/daewoong/userdata/danbi/final_tts_audio'\n",
    "save_path = '/home/daewoong/userdata/danbi/encoder_result'\n",
    "lyric_path = '/home/daewoong/userdata/danbi/final_lyrics_data/'\n",
    "\n",
    "for audio_name in filtered_id_list:\n",
    "  audio, sr = torchaudio.load(f'{audio_path}/{audio_name}.wav')\n",
    "  spec = processor.feature_extractor(audio.mean(dim=0) , sampling_rate=sr, padding_value = 0.0, return_tensors=\"pt\", return_attention_mask = True)\n",
    "  torch.save(spec, f'{save_path}/{audio_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)\n",
    "            the mel spectrogram of the audio\n",
    "        \"\"\"\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1) # (batch_size, n_ctx, n_state)\n",
    "\n",
    "        assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n",
    "        x = (x + self.positional_embedding).to(x.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_path = '/home/daewoong/userdata/danbi/final_tts_audio'\n",
    "save_path = '/home/daewoong/userdata/danbi/encoder_result'\n",
    "lyric_path = '/home/daewoong/userdata/danbi/final_lyrics_data/'\n",
    "each_lyric = '/home/daewoong/userdata/danbi/each_song_lyrics.txt'\n",
    "\n",
    "from data_utils import MinyoDataset, custom_collate_fn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import whisper\n",
    "device = 'cuda'\n",
    "dataset = MinyoDataset(audio_path, lyric_path, processor, filtered_id_list, each_lyric, max_len = 1024)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn, num_workers=8, pin_memory=True)\n",
    "model = whisper.load_model(\"/home/daewoong/userdata/danbi/whisper_pretrain/large-v2.pt\", device='cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.5030e-01, -2.1314e-01, -7.1459e-02,  ..., -4.2323e+00,\n",
       "          -8.2900e-02, -6.4309e-01],\n",
       "         [ 8.5316e-02,  4.7627e-01,  1.7633e-01,  ..., -3.9504e+00,\n",
       "           4.6285e-01, -1.1680e+00],\n",
       "         [ 6.0381e-02,  6.4522e-01, -2.2624e-01,  ..., -3.2380e+00,\n",
       "           1.1161e+00, -1.0413e+00],\n",
       "         ...,\n",
       "         [-8.2729e-03, -4.5520e-03, -1.1558e-02,  ...,  6.3755e-04,\n",
       "          -1.1148e-02, -2.2260e-03],\n",
       "         [ 2.7415e-01,  5.4565e-02, -5.2823e-01,  ...,  1.3546e-02,\n",
       "           3.7851e-01, -6.3340e-02],\n",
       "         [ 5.0610e-01, -1.7489e-01, -3.6949e-01,  ..., -2.1853e-01,\n",
       "           5.1372e-01,  2.2935e-01]]], device='cuda:0',\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model.to(device)\n",
    "batch = next(iter(dataloader))\n",
    "model.encoder(batch[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-4.5030e-01, -2.1314e-01, -7.1459e-02,  ..., -4.2323e+00,\n",
       "         -8.2900e-02, -6.4309e-01],\n",
       "        [ 8.5316e-02,  4.7627e-01,  1.7633e-01,  ..., -3.9504e+00,\n",
       "          4.6285e-01, -1.1680e+00],\n",
       "        [ 6.0381e-02,  6.4522e-01, -2.2624e-01,  ..., -3.2380e+00,\n",
       "          1.1161e+00, -1.0413e+00],\n",
       "        ...,\n",
       "        [-8.2729e-03, -4.5520e-03, -1.1558e-02,  ...,  6.3755e-04,\n",
       "         -1.1148e-02, -2.2260e-03],\n",
       "        [ 2.7415e-01,  5.4565e-02, -5.2823e-01,  ...,  1.3546e-02,\n",
       "          3.7851e-01, -6.3340e-02],\n",
       "        [ 5.0610e-01, -1.7489e-01, -3.6949e-01,  ..., -2.1853e-01,\n",
       "          5.1372e-01,  2.2935e-01]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "audio_path = '/home/daewoong/userdata/danbi/final_tts_audio'\n",
    "save_path = '/home/daewoong/userdata/danbi/encoder_result'\n",
    "lyric_path = '/home/daewoong/userdata/danbi/final_lyrics_data/'\n",
    "each_lyric = '/home/daewoong/userdata/danbi/each_song_lyrics.txt'\n",
    "\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "  for i, batch in tqdm(enumerate(dataloader)):\n",
    "    encoder_value = model.encoder(batch[0].to(device))\n",
    "    encoder_value = encoder_value.squeeze(0).to('cpu')\n",
    "    audio_name = filtered_id_list[i]\n",
    "    torch.save(encoder_value, f'{save_path}/{audio_name}.pt')\n",
    "    break\n",
    "encoder_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500, 1280])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_value.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "  model.encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinyoDataset():\n",
    "  def __init__(self, result_path, lyric_path, processor, filtered_id_list, each_lyric, max_len):#filtered_audio_paths, processor, filtered_txt_paths, each_lyric, sorted_txt_paths, max_len = 1024):\n",
    "    self.encoder_result_paths = Path(result_path)\n",
    "    self.filtered_id_list = filtered_id_list\n",
    "    self.text_process = TextProcessor(filtered_id_list, lyric_path, each_lyric)\n",
    "    self.processor = processor\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def trunaction(self, text, token_text):\n",
    "    words = text.split()\n",
    "    \n",
    "    while len(token_text['input_ids'][0]) > self.max_len:\n",
    "      words = words[:-1]\n",
    "      text = '\\n '.join(words) + '\\n'\n",
    "      token_text = self.processor.tokenizer(text, return_tensors=\"pt\")\n",
    "    return token_text\n",
    "\n",
    "  def around_pad_seqence(self, token_text):\n",
    "    \n",
    "    if len(token_text['input_ids'][0]) < self.max_len:\n",
    "      pad_length = self.max_len - len(token_text['input_ids'][0])\n",
    "      \n",
    "      padding = torch.full((pad_length,), 50257, dtype=torch.long)\n",
    "      attn_padding = torch.full((pad_length,), 0, dtype=torch.long)\n",
    "      \n",
    "      around_text_ids = torch.cat([token_text['input_ids'][0], padding])\n",
    "      around_text_mask = torch.cat([token_text['attention_mask'][0], attn_padding])    \n",
    "      return around_text_ids, around_text_mask\n",
    "\n",
    "    else: \n",
    "      return token_text['input_ids'][0], token_text['attention_mask'][0]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.text_process)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    input_text, around_text, lyric_name = self.text_process[idx]\n",
    "    around_text_org = around_text\n",
    "    audio_encoder_value = torch.load(self.encoder_result_paths / (lyric_name+'.pt'))\n",
    "    # audio, sr = torchaudio.load(self.audio_paths / (lyric_name+'.wav'))\n",
    "    # audio = self.processor.feature_extractor(audio[0] , sampling_rate=sr, padding_value = 0.0, return_tensors=\"pt\", return_attention_mask = True)\n",
    "    input_text = self.processor.tokenizer(input_text, return_tensors=\"pt\")\n",
    "    around_text = self.processor.tokenizer(around_text, return_tensors=\"pt\")\n",
    "    \n",
    "    num_tokens = len(around_text['input_ids'][0])\n",
    "    \n",
    "    if num_tokens > self.max_len:\n",
    "      around_text = self.trunaction(around_text_org, around_text)\n",
    "\n",
    "    around_text_ids, around_text_mask = self.around_pad_seqence(around_text)\n",
    "    return audio_encoder_value, input_text.input_ids[0], input_text.attention_mask[0], around_text_ids, around_text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 11 13:57:33 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:C1:00.0 Off |                  Off |\n",
      "| 55%   82C    P2   260W / 300W |  33388MiB / 49140MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/daewoong/userdata/danbi/encoder_result/lyric3653-4.pt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "encoder_result = Path('/home/daewoong/userdata/danbi/encoder_result')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1500, 1280])\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "emcoder_result = str(list(encoder_result.rglob('*.pt'))[4])\n",
    "emcoder_result\n",
    "result = torch.load(emcoder_result)\n",
    "print(result.shape)\n",
    "if result[0].shape == torch.Size([1280]):\n",
    "  print('hi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
